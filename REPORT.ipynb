{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interviewer Occupation Detection \n",
    "\n",
    "Authors: Ahmed Z. , Cohen H. and Spence S."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "(overview of approach)\n",
    "\n",
    "\n",
    "# Related Work\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing Methods\n",
    "\n",
    "We developed four filters that can be applied to the data. During development of our classification algorithms, we tested on all 16 combinations of those four filters being on or off. For example, the ones and zeroes in the file named \"train-0111.csv\" correspond to whether each filter is on or off. For \"0111\":\n",
    "\n",
    "0 remove_stops. Set to false, do not remove stops words like \"at\" and \"the\".\n",
    "1 stem. Set to true, stem words like \"running\" into \"run\". Used Python's NLTK SnowballStemmer.\n",
    "1 remove_tokens. Set to true - remove all tokens (comma, dash, question mark). Does not remove periods though.\n",
    "1 remove_periods. Set to true - remove all periods.\n",
    "\n",
    "We also applied two more filters regardless of filter settings. All characters outside a (generous) whitelist were removed. Finally, all words were set to lowercase, except for unusual capitalization, like capitals not succeeding a period, or many capitals like in acronyms.\n",
    "\n",
    "Tokens that were not removed were instead replaced by \"_comma\", \"_period\", etc. The script \"cleaner.py\" was used to produce the 16 CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Design and Selection Methods\n",
    "\n",
    "have we designed any features? \n",
    "How about is_interviewer or is_interviewee\n",
    "\n",
    "\n",
    "One major feature in most of our algorithms was the extensive use of ngrams. The Gimli and Pippin approaches especially, when examining text, extract from the text all permutations of 1-grams, 2-grams, 3-grams, etc, up to some limit (usually  4). The \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'utilities'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-953e41adfb63>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mutilities\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"I\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"am\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"a\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"sentence\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"_period\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_ngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_cumulative_ngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'utilities'"
     ]
    }
   ],
   "source": [
    "from stuart.utilities import *\n",
    "words=(\"I\",\"am\",\"a\",\"sentence\",\"_period\")\n",
    "print(get_ngrams(words,3))\n",
    "print(get_cumulative_ngrams(words,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Selection\n",
    "\n",
    "## Baseline\n",
    "\n",
    "Picked a Bernoulli Naive Bayes and made it multiclass using the 1 vs all approach. We also used indicator bag of words rather than a frequency bag of words, this is because the exerpts were short enough that the words will not appear often enough to be counted highly \n",
    "(i have analysis of this.... will put that in here....)\n",
    "\n",
    "Maybe I can also find words that appear with frequency 2 multiple times and add that in as another binary feature?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard\n",
    "\n",
    "Stuarts speil on decision trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced\n",
    "\n",
    "We need to get on with this ASAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization \n",
    "\n",
    "- if any used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter selection\n",
    "\n",
    "- here I can probably do an analysis of which gamma works best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing + Validation \n",
    "\n",
    "- done outside of Kaggle\n",
    "\n",
    "(mention how first two filters raised accuracy generally, while second two didn't do much -Stuart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pros and Cons of approach and methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We\thereby\tstate\tthat\tall\tthe\twork presented\tin this\treport\tis\tthat\tof\tthe\tauthors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statement of Contributions\n",
    "\n",
    "- Zafarali Ahmed\n",
    "- Stuart Spence\n",
    "- Hannah Cohen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
