{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithmic Description for Gimli\n",
    "\n",
    "Consider a training dataset $D$ containing $N$ documents that are classified into categories $C_1, \\ldots, C_k$. The number of documents in each category is given by $N(C_i)~\\forall i~\\in \\{1,k\\}$. A word (or later phrase), $w$, is a word (or n-gram) obtained from the corpus of documents.\n",
    "\n",
    "The number of times the word, $w$, appears in each of the categories is given by $N(w \\land C_i)$. Using the above two estimates we construct a Score, $S(w | C_i) = \\frac{N(w \\land C_i)}{N(C_i)}$, which is the frequency that word $w$ occurs in $C_i$ and can be thought of as  the estimated probability of observing word $w$ in a given category $C_i$. We can combine these to obtain a scoring vector which scores the word in each category.\n",
    "$$S_w = < \\frac{N(w \\land C_1)}{N(C_1)}, \\ldots, \\frac{N(w \\land C_k)}{N(C_k)}> = < S(w | C_1), \\ldots, S(w | C_k) >$$\n",
    "\n",
    "This can also be thought of the frequency of $w$ in each category $C_i$.\n",
    "\n",
    "We can normalize this vector to make it equal to one, but for now let's leave it like this.\n",
    "\n",
    "We now extend our defition of word to account for all consequitive 2-grams and 3-grams from a single observation document, $d\\in D$\n",
    "\n",
    "For a sentence, $d$, the score for $d$ in category $C_i$ is given by a summation of the $n$th power of the score for individual n-grams, $w$ created from $d$: $$S(d | C_i) = \\sum_{\\text{$w \\in$ n-grams of $d$}} S(w | C_i)^n$$\n",
    "\n",
    "The $n$ adds a hyperparameter in our model, which we can tune to improve our prediction accuracy. This can be discussed later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a dataset $D$ which contains interviews of people from two professions: \"musician\", $M$ and \"author\", $A$. The score for words like \"played\" or instruments might be higher in category $M$ than in $A$, while words like \"wrote\" or \"book\" might be higher in category $A$ rather than $M$.\n",
    "\n",
    "Consider the sentence $d=$\"I played my guitar all night long\" $\\in D$, after a bit of transforming (stemming, removing stop words) will obtain \"play guitar all night long\". We now need to classify if this sentence belongs to category $A$ or $M$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score $\\mathcal{S}$ for sentence $d$ is given by the summation of individual scores for each category:\n",
    "$\\mathcal{S}(d) = S_\\text{play}+ S_\\text{guitar} + S_\\text{all} + S_\\text{night} + S_\\text{long} + S_\\text{play guitar} + S_\\text{guitar all} + S_\\text{all night} + S_\\text{night long} + S_\\text{play guitar all} + S_\\text{guitar all night}+ S_\\text{all night long}$ is the summation of 2-d vectors, thus our final vector contains the score for class A and class B:\n",
    "\n",
    "$$\\mathcal{S}(d) = < S(d | C_A),~S(d | C_B)> $$\n",
    "\n",
    "We can note that words like \"play\", \"guitar\", \"play guitar\" will have high scores in their first components, thus we can now use the general decision rule:\n",
    "\n",
    "$$C_{predicted}\\longleftarrow \\underset{C_i \\in Categories} {\\mathrm{argmax}} S(d | C_i)$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
